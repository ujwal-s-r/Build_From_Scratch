{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f78e9551",
   "metadata": {},
   "source": [
    "# Rotary Positional Embeddings (RoPE)\n",
    "\n",
    "To understand RoPE, you must fundamentally change how you view \"position.\"\n",
    "\n",
    "- **Old View (Absolute/Learnable):** Position is a signal we add to the data.\n",
    "- **New View (RoPE):** Position is an orientation in space.\n",
    "\n",
    "## Core Idea of RoPE (one sentence)\n",
    "\n",
    "**RoPE rotates token embeddings in a position-dependent way so that attention naturally understands relative distance.**\n",
    "\n",
    "## Why Rotation Works Better Than Addition\n",
    "\n",
    "### Learnable Embeddings (old way)\n",
    "```\n",
    "embedding = word_vector + position_vector\n",
    "```\n",
    "\n",
    "**Problem:**\n",
    "- Position info is mixed with meaning\n",
    "- Hard to infer distance\n",
    "\n",
    "### RoPE (new way)\n",
    "```\n",
    "embedding = ROTATE(word_vector, angle = position)\n",
    "```\n",
    "\n",
    "**Key advantage:**\n",
    "- Rotation preserves length\n",
    "- Only changes direction\n",
    "- Relative angle difference = relative position difference\n",
    "\n",
    "## How Rotation is Done (simple math, no fear)\n",
    "\n",
    "Each embedding is split into pairs:\n",
    "```\n",
    "[x1, x2], [x3, x4], [x5, x6], ...\n",
    "```\n",
    "\n",
    "Each pair acts like a **2D vector**.\n",
    "\n",
    "### Rotation Formula:\n",
    "```\n",
    "[x', y'] = [ x*cosθ - y*sinθ ,\n",
    "             x*sinθ + y*cosθ ]\n",
    "```\n",
    "\n",
    "**θ (theta) depends on:**\n",
    "- Token position\n",
    "- Embedding dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2768e21",
   "metadata": {},
   "source": [
    "## The Math: Rotation Matrices\n",
    "\n",
    "Imagine a 2D vector $(x_1, x_2)$. In the complex plane, this is $x_1 + i x_2$.\n",
    "\n",
    "To rotate this vector by an angle $\\theta$, we multiply it by $e^{i\\theta}$.\n",
    "\n",
    "### How RoPE Works\n",
    "\n",
    "RoPE applies this to the high-dimensional embedding vector by **chopping it into chunks of 2**.\n",
    "\n",
    "If $d_{model} = 512$, we treat it as **256 pairs** of coordinates.\n",
    "\n",
    "### The Rotation Formula\n",
    "\n",
    "For a token at position $m$, we rotate the pair $(x_1, x_2)$ by an angle $m \\theta$:\n",
    "\n",
    "$$\\begin{pmatrix} x'_1 \\\\ x'_2 \\end{pmatrix} = \\begin{pmatrix} \\cos(m\\theta) & -\\sin(m\\theta) \\\\ \\sin(m\\theta) & \\cos(m\\theta) \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$$\n",
    "\n",
    "### The Magic Property ✨\n",
    "\n",
    "If you take the dot product of a **Query** rotated by $m\\theta$ and a **Key** rotated by $n\\theta$, the math simplifies beautifully:\n",
    "\n",
    "$$\\text{Score} = \\text{OriginalScore} \\times \\cos((m - n)\\theta)$$\n",
    "\n",
    "**The absolute positions $m$ and $n$ disappear!** Only the relative distance $(m - n)$ remains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5326ff8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
