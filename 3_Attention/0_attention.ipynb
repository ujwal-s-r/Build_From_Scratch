{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9148cc",
   "metadata": {},
   "source": [
    "## The Components:\n",
    "\n",
    "- **Query ($Q$):** What I am looking for. (e.g., \"The cat sat on the...\")\n",
    "- **Key ($K$):** The label or \"tag\" of the information stored. (e.g., \"mat\")\n",
    "- **Value ($V$):** The actual content/information. (e.g., The vector embedding for \"mat\")\n",
    "\n",
    "## The Process:\n",
    "\n",
    "1. **Similarity:** Compare $Q$ with every $K$ (Dot Product).\n",
    "2. **Probability:** Turn scores into percentages (Softmax).\n",
    "3. **Retrieval:** Multiply percentages by $V$.\n",
    "\n",
    "## The Math\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "- **$QK^T$:** The Dot Product. Measures alignment. High score = High similarity.\n",
    "- **$\\sqrt{d_k}$:** The Scaling Factor. (We will verify why this is needed below).\n",
    "- **$\\text{Softmax}$:** Normalizes scores so they sum to 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412db7bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7f8c399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2.1 Attention Math Check ---\n",
      "Scores Shape: torch.Size([1, 1, 3, 3])\n",
      "Output Shape: torch.Size([1, 1, 3, 4])\n",
      "\n",
      "Attention Weights (Row sum must be 1.0):\n",
      "tensor([[0.2053, 0.3439, 0.4508],\n",
      "        [0.2956, 0.3737, 0.3308],\n",
      "        [0.5547, 0.2591, 0.1862]])\n",
      "Row Sum: tensor([1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    \"\"\"\n",
    "    Computes the scaled dot product attention.\n",
    "    Args:\n",
    "        q: Queries (Batch, Heads, Seq_Len_Q, Dim_Head)\n",
    "        k: Keys    (Batch, Heads, Seq_Len_K, Dim_Head)\n",
    "        v: Values  (Batch, Heads, Seq_Len_K, Dim_Head)\n",
    "        mask: Optional mask (e.g., for causal masking)\n",
    "    \"\"\"\n",
    "    d_k = q.size(-1) # Dimension of the key head\n",
    "    \n",
    "    # 1. Similarity Scores (Q @ K_transpose)\n",
    "    # Shape: (Batch, Heads, Seq_Len_Q, Seq_Len_K)\n",
    "    # We transpose the last two dimensions of K to align for multiplication\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1))\n",
    "    \n",
    "    # 2. Scaling (The Stability Key)\n",
    "    scores = scores / math.sqrt(d_k)\n",
    "    \n",
    "    # 3. Masking (Optional - we will use this later)\n",
    "    if mask is not None:\n",
    "        # Replace 0s (masked positions) with -infinity so softmax makes them 0\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # 4. Attention Weights (Softmax)\n",
    "    # Convert scores to probabilities (0.0 to 1.0)\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # 5. Weighted Aggregation (Weights @ V)\n",
    "    output = torch.matmul(attn_weights, v)\n",
    "    \n",
    "    return output, attn_weights\n",
    "\n",
    "# --- VERIFICATION LAB ---\n",
    "def verify_attention():\n",
    "    print(\"--- 2.1 Attention Math Check ---\")\n",
    "    \n",
    "    # Setup standard dimensions\n",
    "    batch_size = 1\n",
    "    heads = 1\n",
    "    seq_len = 3\n",
    "    d_k = 4 # Tiny dimension for easy reading\n",
    "    \n",
    "    # Create fake data\n",
    "    # Query: Looking for something specific\n",
    "    q = torch.randn(batch_size, heads, seq_len, d_k)\n",
    "    k = torch.randn(batch_size, heads, seq_len, d_k)\n",
    "    v = torch.randn(batch_size, heads, seq_len, d_k)\n",
    "    \n",
    "    output, weights = scaled_dot_product_attention(q, k, v)\n",
    "    \n",
    "    print(f\"Scores Shape: {weights.shape}\")\n",
    "    print(f\"Output Shape: {output.shape}\")\n",
    "    print(\"\\nAttention Weights (Row sum must be 1.0):\")\n",
    "    print(weights[0,0]) \n",
    "    print(\"Row Sum:\", weights[0,0].sum(dim=-1))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    verify_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d59a3a",
   "metadata": {},
   "source": [
    "## Why Scaling? (The \"Hot\" Softmax Problem)\n",
    "\n",
    "You asked why we divide by $\\sqrt{d_k}$. Let's use simple numbers.\n",
    "\n",
    "### The Softmax Function:\n",
    "\n",
    "Softmax turns numbers into probabilities.\n",
    "\n",
    "- **Input:** [2, 1] → **Softmax:** [0.73, 0.27] (Nice, soft mix)\n",
    "- **Input:** [20, 10] → **Softmax:** [0.99995, 0.00005] (Extreme, hard spike)\n",
    "\n",
    "### The Problem with High Dimensions:\n",
    "\n",
    "In deep learning, our vectors are long (e.g., 512 numbers).\n",
    "\n",
    "When you do a Dot Product of two long vectors, the result is a **Sum**.\n",
    "\n",
    "$1 \\times 1 + 1 \\times 1 + ...$ (512 times) = 512.\n",
    "\n",
    "The numbers get huge just because the vectors are long.\n",
    "\n",
    "If we feed huge numbers (like 512) into Softmax, it panics. It outputs 1.0 for the winner and 0.0 for everyone else.\n",
    "\n",
    "**Result:** The model becomes \"arrogant.\" It only looks at one thing and ignores everything else. The gradients die.\n",
    "\n",
    "### The Fix (Scaling):\n",
    "\n",
    "We divide the huge number by the square root of the length ($\\sqrt{512} \\approx 22$).\n",
    "\n",
    "$512 / 22 \\approx 23$. (Still big, but manageable).\n",
    "\n",
    "Usually, it brings the variance back down to ~1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4622b8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Why Scaling Matters ---\n",
      "\n",
      "1. Small Scores: [2.0, 1.0, 0.5]\n",
      "   Softmax Output: [0.6285316944122314, 0.23122389614582062, 0.14024437963962555]\n",
      "   -> Result: Nice distribution. The model considers all options.\n",
      "\n",
      "2. Huge Scores (Unscaled): [200.0, 100.0, 50.0]\n",
      "   Softmax Output: [1.0, 3.783505853677006e-44, 0.0]\n",
      "   -> Result: [1.0, 0.0, 0.0]. The model is 'arrogant'. Gradient is DEAD.\n",
      "\n",
      "3. Scaled Scores: [2.0, 1.0, 0.5]\n",
      "   Softmax Output: [0.6285316944122314, 0.23122389614582062, 0.14024437963962555]\n",
      "   -> Result: Back to a nice distribution! Gradients can flow.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "def simple_scaling_demo():\n",
    "    print(\"--- Why Scaling Matters ---\")\n",
    "    \n",
    "    # 1. Imagine a Dot Product result (Similarity Scores)\n",
    "    # Let's say we have 3 keys.\n",
    "    \n",
    "    # CASE A: Small numbers (Simulating a small network)\n",
    "    small_scores = torch.tensor([2.0, 1.0, 0.5])\n",
    "    print(f\"\\n1. Small Scores: {small_scores.tolist()}\")\n",
    "    \n",
    "    # Apply Softmax\n",
    "    probs_small = F.softmax(small_scores, dim=0)\n",
    "    print(f\"   Softmax Output: {probs_small.tolist()}\")\n",
    "    print(\"   -> Result: Nice distribution. The model considers all options.\")\n",
    "\n",
    "    # CASE B: Huge numbers (Simulating a real Transformer without scaling)\n",
    "    # In 512-dim space, dot products naturally become huge (e.g., 50, 100)\n",
    "    huge_scores = torch.tensor([200.0, 100.0, 50.0])\n",
    "    print(f\"\\n2. Huge Scores (Unscaled): {huge_scores.tolist()}\")\n",
    "    \n",
    "    # Apply Softmax\n",
    "    probs_huge = F.softmax(huge_scores, dim=0)\n",
    "    print(f\"   Softmax Output: {probs_huge.tolist()}\")\n",
    "    print(\"   -> Result: [1.0, 0.0, 0.0]. The model is 'arrogant'. Gradient is DEAD.\")\n",
    "    \n",
    "    # CASE C: The Fix (Scaling)\n",
    "    # We divide by a factor (let's say 100 to simulate sqrt(d_k))\n",
    "    scale_factor = 100.0\n",
    "    scaled_scores = huge_scores / scale_factor\n",
    "    print(f\"\\n3. Scaled Scores: {scaled_scores.tolist()}\")\n",
    "    \n",
    "    # Apply Softmax\n",
    "    probs_scaled = F.softmax(scaled_scores, dim=0)\n",
    "    print(f\"   Softmax Output: {probs_scaled.tolist()}\")\n",
    "    print(\"   -> Result: Back to a nice distribution! Gradients can flow.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    simple_scaling_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb732c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
